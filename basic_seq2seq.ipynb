{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Seq2Seq Practice\n",
    "### Source code come from  :  '從零開始的 Sequence to Sequence ' \n",
    "Article： http://zake7749.github.io/2017/09/28/Sequence-to-Sequence-tutorial/ <br>\n",
    "Github: https://github.com/zake7749/Sequence-to-Sequence-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq Moder!\n",
    "![](./image/seq2seq.png)\n",
    "\n",
    "##### Seq2Seq Model: 由兩個Sequential model組成，輸入和輸出都可以是序列資料，也被稱作 Encoder-Decoder framework。\n",
    "1. Sequential model擅長處理有序列特徵的資料(文字,語音,時間序)，模型常見的基本組成就是RNN、LSTM、GRU。\n",
    "2. Encoder: 把輸入的文字轉換成機器理解的context vector。\n",
    "3. Decoder: 把context vector轉換成我們能理解的文字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "'''Self define package by author (check author's github)'''\n",
    "from dataset.DataHelper import DataTransformer\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Seq2Seq Model (Combine encoder & decoder)\n",
    "1. Input encoder & decoder model and declared <br>\n",
    "2. In forward: input date / runnung encoder forward / runnung decoder forward\n",
    "3. Evaluation: Using test data => Running Encoder forward => Running Decoder evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        # Input Encoder、Decoder model and declare.\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Input training data\n",
    "        input_vars, input_lengths = inputs\n",
    "        \n",
    "        #Running encoder\n",
    "        encoder_outputs, encoder_hidden = self.encoder.forward(input_vars, input_lengths)\n",
    "        \n",
    "        # Running decoder\n",
    "        decoder_outputs, decoder_hidden = self.decoder.forward(context_vector=encoder_hidden, targets=targets)\n",
    "        return decoder_outputs, decoder_hidden\n",
    "\n",
    "    \n",
    "    \n",
    "    def evaluation(self, inputs):\n",
    "        # Input test data\n",
    "        input_vars, input_lengths = inputs\n",
    "        \n",
    "        # \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_vars, input_lengths)\n",
    "        decoded_sentence = self.decoder.evaluation(context_vector=encoder_hidden)\n",
    "        return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Model\n",
    "將一組序列(input)用 Embedding 轉成向量，並在 RNN 最後一個時間點的輸出 hidden 做為 context vector。 <br>\n",
    "\n",
    "* 補充 PackedSequence 物件：<br>\n",
    "1. 在 Recurrent neural network 裡，由於每筆資料的 input 和 output 在長度會有所不同，無法用 batch 的方式來 train ，在 pytorch 有一個特別的 class 叫 PackedSequence，用來幫忙解決這個問題。<br>\n",
    "http://www.cnblogs.com/lindaxin/p/8052043.html <br>\n",
    "![](./image/padd.png)\n",
    "2. 用 torch.nn.utils.rnn.pack_padded_sequence將 Variable 轉換成 PackedSequence  ;  如果要轉換回 Variable ，要用torch.nn.utils.rnn.pad_packed_sequence這個函式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, output_size):\n",
    "        \"\"\"Define layers for a vanilla rnn encoder\"\"\"\n",
    "        super(VanillaEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, output_size) # GRU: one kind of rnn model\n",
    "\n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        # input to vector(variable)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        # vector(variable) to packed sequence (become same length)\n",
    "        packed = pack_padded_sequence(embedded, input_lengths)\n",
    "        # Runnung RNN\n",
    "        packed_outputs, hidden = self.gru(packed, hidden)\n",
    "        # packed sequence to vector(variable) \n",
    "        outputs, output_lengths = pad_packed_sequence(packed_outputs)\n",
    "        return outputs, hidden\n",
    "\n",
    "    def forward_a_sentence(self, inputs, hidden=None):\n",
    "        \"\"\"Deprecated, forward 'one' sentence at a time which is bad for gpu utilization\"\"\"\n",
    "        embedded = self.embedding(inputs)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Model\n",
    "\n",
    "##### 和 Decoder 類似 Encoder 只是他的 input 除了來自 Encoder 之外，每一個時間的的 output 也會變成下一個時間點的input，以下重點：\n",
    "\n",
    "* Flow1. first input: SOS (Start of sentence) <br> \n",
    "* Flow2. first hidden : Pass the context vector <br>\n",
    "* Flow3. Decoder每個時間點的output當作下個時間點input，利用 [forward_step] 來執行RNN，和 Encoder類似都是 GRU，只是多出每個時間點的 output。<br>\n",
    "* 補充訓練小技巧：teacher_forcing_ratio 是個常數機率（本例子設0.5），用於隨機將 Decoder下個時間的的 input換成是真正 Label，以幫助訓練的穩定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, max_length, teacher_forcing_ratio, sos_id, use_cuda):\n",
    "        \"\"\"Define layers for a vanilla rnn decoder\"\"\"\n",
    "        super(VanillaDecoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.log_softmax = nn.LogSoftmax()  # work with NLLLoss = CrossEntropyLoss\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.sos_id = sos_id\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        \n",
    "    def forward_step(self, inputs, hidden):\n",
    "        '''Run GRU in each time step:\n",
    "           和 Encoder類似都是 GRU，只是多出每個時間點的 output'''\n",
    "        # inputs: (time_steps=1, batch_size)\n",
    "        batch_size = inputs.size(1)\n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded.view(1, batch_size, self.hidden_size)  # S = T(1) x B x N\n",
    "        rnn_output, hidden = self.gru(embedded, hidden)  # S = T(1) x B x H\n",
    "        rnn_output = rnn_output.squeeze(0)  # squeeze the time dimension\n",
    "        output = self.log_softmax(self.out(rnn_output))  # S = B x O\n",
    "        # self.out： nn.Linear(data) / Ax+b \n",
    "        # self.log_softmax = nn.LogSoftmax()  # work with NLLLoss = CrossEntropyLoss\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "    ### 重點流程：\n",
    "    def forward(self, context_vector, targets):\n",
    "        # Prepare variable for decoder on time_step_0\n",
    "        target_vars, target_lengths = targets\n",
    "        batch_size = context_vector.size(1)\n",
    "        \n",
    "        ''' Flow1. \n",
    "            first input: SOS (Start of sentence) ''' \n",
    "        decoder_input = Variable(torch.LongTensor([[self.sos_id] * batch_size]))\n",
    "        \n",
    "        \n",
    "        ''' Flow2.\n",
    "            first hidden : context vector/ come frome Encoder '''\n",
    "        decoder_hidden = context_vector\n",
    "\n",
    "        max_target_length = max(target_lengths)\n",
    "        decoder_outputs = Variable(torch.zeros(\n",
    "            max_target_length,\n",
    "            batch_size,\n",
    "            self.output_size\n",
    "        ))  # (time_steps, batch_size, vocab_size)\n",
    "\n",
    "        if self.use_cuda:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "            decoder_outputs = decoder_outputs.cuda()\n",
    "\n",
    "            \n",
    "        '''補充訓練小技巧：\n",
    "          teacher_forcing_ratio 是個常數機率（本例子設0.5），用於隨機將 Decoder下個時間的的 input\n",
    "          換成是真正 Label，以幫助訓練的穩定性。\n",
    "        ''' \n",
    "        use_teacher_forcing = True if random.random() > self.teacher_forcing_ratio else False\n",
    "        \n",
    "        \n",
    "        ''' Flow3.\n",
    "        Decoder每個時間點的 output 當作下個時間點 input，利用 [forward_step] 來執行RNN，和 Encoder 類似\n",
    "        都是 GRU，只是多出每個時間點的 output。'''\n",
    "        for t in range(max_target_length):\n",
    "            decoder_outputs_on_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs[t] = decoder_outputs_on_t\n",
    "            \n",
    "            # 同上訓練小技巧\n",
    "            if use_teacher_forcing:\n",
    "                decoder_input = target_vars[t].unsqueeze(0)\n",
    "                # 一定機率給真實Label回去訓練\n",
    "            else:\n",
    "                decoder_input = self._decode_to_index(decoder_outputs_on_t)\n",
    "                # 一定機率給 decoder_outputs_on_t\n",
    "\n",
    "        return decoder_outputs, decoder_hidden\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def evaluation(self, context_vector):\n",
    "        batch_size = context_vector.size(1) # get the batch size\n",
    "        decoder_input = Variable(torch.LongTensor([[self.sos_id] * batch_size]))\n",
    "        decoder_hidden = context_vector\n",
    "\n",
    "        decoder_outputs = Variable(torch.zeros(\n",
    "            self.max_length,\n",
    "            batch_size,\n",
    "            self.output_size\n",
    "        ))  # (time_steps, batch_size, vocab_size)\n",
    "\n",
    "        if self.use_cuda:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "            decoder_outputs = decoder_outputs.cuda()\n",
    "\n",
    "        # Unfold the decoder RNN on the time dimension\n",
    "        for t in range(self.max_length):\n",
    "            decoder_outputs_on_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs[t] = decoder_outputs_on_t\n",
    "            decoder_input = self._decode_to_index(decoder_outputs_on_t)  # select the former output as input\n",
    "\n",
    "        return self._decode_to_indices(decoder_outputs)\n",
    "     \n",
    "    \n",
    "\n",
    "    def _decode_to_index(self, decoder_output):\n",
    "        \"\"\"\n",
    "        evaluate on the logits, get the index of top1:\n",
    "        param decoder_output: S = B x V or T x V\n",
    "        \"\"\"\n",
    "        value, index = torch.topk(decoder_output, 1)\n",
    "        # Returns the k largest elements of the given input tensor along a given dimension.\n",
    "        index = index.transpose(0, 1)  # S = 1 x B, 1 is the index of top1 class\n",
    "        if self.use_cuda:\n",
    "            index = index.cuda()\n",
    "        return index\n",
    "    \n",
    "    \n",
    "\n",
    "    def _decode_to_indices(self, decoder_outputs):\n",
    "        \"\"\"\n",
    "        Evaluate on the decoder outputs(logits), find the top 1 indices.\n",
    "        Please confirm that the model is on evaluation mode if dropout/batch_norm layers have been added\n",
    "        :param decoder_outputs: the output sequence from decoder, shape = T x B x V \n",
    "        \"\"\"\n",
    "        decoded_indices = []\n",
    "        batch_size = decoder_outputs.size(1)\n",
    "        decoder_outputs = decoder_outputs.transpose(0, 1)  # S = B x T x V\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            top_ids = self._decode_to_index(decoder_outputs[b])\n",
    "            decoded_indices.append(top_ids.data[0])\n",
    "        return decoded_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Training Object\n",
    "1. init: initializing seq2seq model, dataset information, optimizer setting\n",
    "2. train method: Training seq2seq model [num_epochs] times, with [mini_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, model, data_transformer, learning_rate, use_cuda,\n",
    "                 checkpoint_name=config.checkpoint_name,\n",
    "                 teacher_forcing_ratio=config.teacher_forcing_ratio):\n",
    "\n",
    "        self.model = model #seq2seq model\n",
    "\n",
    "        # record some information about dataset\n",
    "        self.data_transformer = data_transformer\n",
    "        self.vocab_size = self.data_transformer.vocab_size\n",
    "        self.PAD_ID = self.data_transformer.PAD_ID\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # optimizer setting\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer= torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.criterion = torch.nn.NLLLoss(ignore_index=self.PAD_ID, size_average=True)\n",
    "\n",
    "        self.checkpoint_name = checkpoint_name\n",
    "        \n",
    "        \n",
    "\n",
    "    def train(self, num_epochs, batch_size, pretrained=False):\n",
    "\n",
    "        if pretrained:\n",
    "            self.load_model()\n",
    "\n",
    "        step = 0\n",
    "\n",
    "        for epoch in range(0, num_epochs):\n",
    "            mini_batches = self.data_transformer.mini_batches(batch_size=batch_size)\n",
    "            for input_batch, target_batch in mini_batches:\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Call seq2seq model to training\n",
    "                decoder_outputs, decoder_hidden = self.model(input_batch, target_batch)\n",
    "\n",
    "                # calculate the loss and back prop.\n",
    "                cur_loss = self.get_loss(decoder_outputs, target_batch[0])\n",
    "\n",
    "                # logging\n",
    "                step += 1\n",
    "                if step % 50 == 0:\n",
    "                    print(\"Step:\", step, \"loss of char: \", cur_loss.data[0])\n",
    "                    self.save_model()\n",
    "                cur_loss.backward()\n",
    "\n",
    "                # optimizing parameter\n",
    "                # torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "                self.optimizer.step()\n",
    "        self.save_model()\n",
    "\n",
    "        \n",
    "    def masked_nllloss(self):\n",
    "        # Deprecated in PyTorch 2.0, can be replaced by ignore_index\n",
    "        # define the masked NLLoss\n",
    "        weight = torch.ones(self.vocab_size)\n",
    "        weight[self.PAD_ID] = 0\n",
    "        if self.use_cuda:\n",
    "            weight = weight.cuda()\n",
    "        return torch.nn.NLLLoss(weight=weight).cuda()\n",
    "    \n",
    "\n",
    "    def get_loss(self, decoder_outputs, targets):\n",
    "        b = decoder_outputs.size(1)\n",
    "        t = decoder_outputs.size(0)\n",
    "        targets = targets.contiguous().view(-1)  # S = (B*T)\n",
    "        decoder_outputs = decoder_outputs.view(b * t, -1)  # S = (B*T) x V\n",
    "        return self.criterion(decoder_outputs, targets)\n",
    "    \n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), self.checkpoint_name)\n",
    "        print(\"Model has been saved as %s.\\n\" % self.checkpoint_name)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model.load_state_dict(torch.load(self.checkpoint_name))\n",
    "        print(\"Pretrained model has been loaded.\\n\")\n",
    "\n",
    "    def tensorboard_log(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "    def evaluate(self, words):\n",
    "        # make sure that words is list\n",
    "        if type(words) is not list:\n",
    "            words = [words]\n",
    "\n",
    "        # transform word to index-sequence\n",
    "        eval_var = self.data_transformer.evaluation_batch(words=words)\n",
    "        decoded_indices = self.model.evaluation(eval_var)\n",
    "        results = []\n",
    "        for indices in decoded_indices:\n",
    "            results.append(self.data_transformer.vocab.indices_to_sequence(indices))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Training Model \n",
    "1. Declare encoder model\n",
    "2. Declare decoder model\n",
    "3. Declare seq2seq midel (by encoder & decoder model)\n",
    "4. Declare training object\n",
    "5. Running training object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Self defined package by author (check author's github)\n",
    "    from dataset.DataHelper import DataTransformer\n",
    "    from config import config\n",
    "    '''\n",
    "    data_transformer = DataTransformer(config.dataset_path, use_cuda=config.use_cuda)\n",
    "\n",
    "    # 1. Declare encoder model\n",
    "    vanilla_encoder = VanillaEncoder(vocab_size=data_transformer.vocab_size,\n",
    "                                     embedding_size=config.encoder_embedding_size,\n",
    "                                     output_size=config.encoder_output_size)\n",
    "    # 2. Declare decoder model\n",
    "    vanilla_decoder = VanillaDecoder(hidden_size=config.decoder_hidden_size,\n",
    "                                     output_size=data_transformer.vocab_size,\n",
    "                                     max_length=data_transformer.max_length,\n",
    "                                     teacher_forcing_ratio=config.teacher_forcing_ratio,\n",
    "                                     sos_id=data_transformer.SOS_ID,\n",
    "                                     use_cuda=config.use_cuda)\n",
    "    if config.use_cuda:\n",
    "        vanilla_encoder = vanilla_encoder.cuda()\n",
    "        vanilla_decoder = vanilla_decoder.cuda()\n",
    "        \n",
    "\n",
    "    # 3. Declare seq2seq midel (by encoder & decoder model)\n",
    "    seq2seq = Seq2Seq(encoder=vanilla_encoder,\n",
    "                      decoder=vanilla_decoder)\n",
    "\n",
    "    # 4. Declare training object\n",
    "    trainer = Trainer(seq2seq, data_transformer, config.learning_rate, config.use_cuda)\n",
    "    \n",
    "    # 5. Running training object\n",
    "    trainer.train(num_epochs=config.num_epochs, batch_size=config.batch_size, pretrained=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting to run    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference:\n",
    "* 科技大擂台 Pytorch Seq2Seq 篇: \n",
    "https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305df38a7c015e194f22f8015b\n",
    "\n",
    "* PyTorch Document: \n",
    "http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.htm\n",
    "\n",
    "###### ==> Further reading: Conbine Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from dataset.DataHelper import DataTransformer\n",
    "import torch.nn as nn\n",
    "from config import config\n",
    "data_transformer = DataTransformer(config.dataset_path, use_cuda=config.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_size:  256\n",
      "vocab_size    :  30\n"
     ]
    }
   ],
   "source": [
    "embedding_size=config.encoder_embedding_size\n",
    "vocab_size=data_transformer.vocab_size\n",
    "text_data = data_transformer.mini_batches(batch_size=100)\n",
    "#mini_batches = data_transformer.mini_batches(batch_size=batch_size)\n",
    "print('embedding_size: ', embedding_size)\n",
    "print('vocab_size    : ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.LongTensor\u001b[0m), but expected (torch.FloatTensor source, int dim, torch.LongTensor index)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-b0e954079482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input_batch:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.LongTensor\u001b[0m), but expected (torch.FloatTensor source, int dim, torch.LongTensor index)"
     ]
    }
   ],
   "source": [
    "EM = nn.Embedding(vocab_size, embedding_size)\n",
    "for input_batch, target_batch in text_data:\n",
    "    input_vars, input_lengths = input_batch\n",
    "    embedded = EM(input_vars)\n",
    "    pack = pack_padded_sequence(embedded, input_lengths, batch_first=True)\n",
    "    print('input_batch:',input_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PackedSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  1\n",
      "  2\n",
      "  3\n",
      "\n",
      "(1 ,.,.) = \n",
      "  4\n",
      "  5\n",
      "  6\n",
      "[torch.LongTensor of size 2x3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "\n",
    "x = torch.LongTensor([[1,2,3], [4,5,6]]).view(2, 3, 1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=\n",
      " 1\n",
      " 4\n",
      " 2\n",
      " 3\n",
      "[torch.LongTensor of size 4x1]\n",
      ", batch_sizes=[2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "lens = [3, 1]\n",
    "y = pack_padded_sequence(x, lens, batch_first=True)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths :  [3 3 3]\n",
      "batch_pt :  Variable containing:\n",
      "(0 ,.,.) = \n",
      "  1  1  1  1\n",
      "  2  2  2  2\n",
      "  3  3  3  3\n",
      "  4  0  0  4\n",
      "  0  0  0  5\n",
      "\n",
      "(1 ,.,.) = \n",
      "  1  1  1  1\n",
      "  2  2  2  2\n",
      "  3  3  3  3\n",
      "  4  0  0  4\n",
      "  0  0  0  5\n",
      "\n",
      "(2 ,.,.) = \n",
      "  1  1  1  1\n",
      "  2  2  2  2\n",
      "  3  3  3  3\n",
      "  4  0  0  4\n",
      "  0  0  0  5\n",
      "[torch.FloatTensor of size 3x5x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Get data\n",
    "single_sample = np.array([[1, 2, 3, 4, 0], \n",
    "                          [1, 2, 3, 0, 0], \n",
    "                          [1, 2, 3, 0, 0], \n",
    "                          [1, 2, 3, 4, 5]], dtype='float32')  # (features, max_len)\n",
    "single_sample = single_sample.T  # (max_len, features)\n",
    "batch = np.array([single_sample] * 3)  # (batch_size, max_len, features)\n",
    "\n",
    "lengths = np.array([3, 3, 3])\n",
    "print('lengths : ', lengths)\n",
    "# Move to pytorch\n",
    "batch_pt = Variable(torch.from_numpy(batch))\n",
    "print('batch_pt : ', batch_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=Variable containing:\n",
       "    1     1     1     1\n",
       "    1     1     1     1\n",
       "    1     1     1     1\n",
       "    2     2     2     2\n",
       "    2     2     2     2\n",
       "    2     2     2     2\n",
       "    3     3     3     3\n",
       "    3     3     3     3\n",
       "    3     3     3     3\n",
       "[torch.FloatTensor of size 9x4]\n",
       ", batch_sizes=[3, 3, 3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pack the sequence\n",
    "xp = pack_padded_sequence(batch_pt, lengths, batch_first=True)\n",
    "xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 50\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.7222  0.2799 -1.4216  0.0531  0.9681  1.1352 -0.5463  1.5978  1.5108  0.0109\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "\n",
    "intValue = int(3)\n",
    "\n",
    "#using nn.Embedding to get 2*5 vector\n",
    "embeds = nn.Embedding(2,5)\n",
    "if intValue == 1:\n",
    "  print(embeds.weight)\n",
    "\n",
    "#modify 2*5 vector \n",
    "embeds.weight.data = torch.ones(2,5)\n",
    "if intValue == 2:\n",
    "  print(embeds.weight)\n",
    "\n",
    "#get 50th word vector of 100*10 vector\n",
    "embeds = nn.Embedding(100, 10)\n",
    "print(Variable(torch.LongTensor([50])))\n",
    "single_word_embed = embeds(Variable(torch.LongTensor([50])))\n",
    "if intValue == 3:\n",
    "  print(single_word_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
